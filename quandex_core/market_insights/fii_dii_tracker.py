"""
NSE FII/DII Institutional Flow Tracker - Improved Version
Robust API and Playwright scraping with anti-automation and fallback strategies.
"""

import asyncio
import polars as pl
import duckdb
from datetime import datetime, timedelta, date
import json
import random
import logging
from loguru import logger
import sys
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Error as PlaywrightError
import time
import re
from typing import List, Dict, Any, Union, Optional

# --- Setup Project Path ---
project_root = Path(__file__).parent.parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

try:
    from quandex_core.config import config
except ImportError:
    # Fallback configuration
    class ConfigFallback:
        class data:
            duckdb_path = project_root / "data_vault" / "market_boards" / "quandex.duckdb"
        class scraping:
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
            ]
            max_retries = 3
            retry_delay = 2
            request_timeout = 15
    config = ConfigFallback()

# --- Logging Setup ---
log_dir = project_root / "logs"
try:
    log_dir.mkdir(parents=True, exist_ok=True)
except Exception as e:
    logger.error(f"Log directory error: {e}")

logger.remove()
logger.add(
    sys.stdout,
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {module}:{line} - {message}"
)
logger.add(
    str(log_dir / "fii_dii_scraper.log"),
    level="DEBUG",
    rotation="10 MB",
    retention="30 days",
    compression="zip",
    enqueue=True
)
logger.info("Logging system initialized")
logger.info(f"DuckDB path: {config.data.duckdb_path}")

class NSE_FII_DII_Scraper:
    def __init__(self):
        """Initialize scraper with robust configuration"""
        try:
            self.db_path = str(config.data.duckdb_path)
            self.api_url = "https://www.nseindia.com/api/fiidiiTradeReact"
            self.home_url = "https://www.nseindia.com"
            self.css_selectors = [
                'table.fii-dii-table',
                'table.table-fii-dii',
                'div#fiiDiiData table',
                'table'
            ]
            self.user_agents = config.scraping.user_agents
            self.max_retries = config.scraping.max_retries
            self.retry_delay = config.scraping.retry_delay
            self.timeout = config.scraping.request_timeout
            self.html_url = config.scraping.nse_fii_dii_html_url # Added html_url
            self.HTML_TABLE_SELECTOR = ".fii-dii-table" # Added table selector, can be made configurable later
            self.session_counter = 0
            self.max_sessions = 3
            # Example headers - REPLACE with your actual headers from browser DevTools
            self.nse_headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.nseindia.com/',
                'Connection': 'keep-alive',
                'Cookie': '',  # <-- REPLACE with your actual cookie
                'X-Requested-With': 'XMLHttpRequest'
            }
            logger.info("FII/DII scraper initialized")
        except Exception as e:
            logger.error(f"Initialization failed: {e}")
            raise

    async def scrape_with_api(self) -> Optional[pl.DataFrame]:
        """Scrape using official NSE API with robust session handling, asynchronously."""
        try:
            # Create session object here; it will be passed to thread-executed functions
            session = requests.Session()
            session.headers.update(self.nse_headers) # Initial headers

            logger.debug(f"Initial request headers: {session.headers}")

            # Define synchronous blocking functions to be run in threads
            def _fetch_home_page():
                # This function will run in a separate thread
                resp = session.get(self.home_url, timeout=self.timeout)
                resp.raise_for_status() # Check for HTTP errors
                return resp

            def _fetch_api_data():
                # This function will run in a separate thread, using the same session
                resp = session.get(self.api_url, timeout=self.timeout)
                # No explicit raise_for_status here, will check status code below
                return resp

            # Visit homepage first to establish session/cookies
            try:
                home_response = await asyncio.to_thread(_fetch_home_page)
                # session object now contains cookies from home_response
                logger.debug(f"Session cookies after home page: {session.cookies.get_dict()}")
                logger.info("NSE session established via home page.")
                # The session object itself is updated with cookies.
                # The self.nse_headers update is for future instantiations of the scraper,
                # not strictly necessary for the current session's subsequent .get() calls.
                # Let's remove the direct session.cookies.get_dict() call here to avoid mock issues if that's the cause.
                # If cookies are correctly handled by the session object, this explicit step isn't needed for functionality.
                # latest_cookies = session.cookies.get_dict()
                # if latest_cookies:
                #     self.nse_headers['Cookie'] = '; '.join(f"{k}={v}" for k, v in latest_cookies.items())

            except Exception as e:
                logger.warning(f"Home page fetch for session setup failed: {e}")
                # Depending on strictness, might return None or try API anyway
                return None

            # Now try the API with the established session (cookies are automatically handled by the session object)
            api_response = await asyncio.to_thread(_fetch_api_data)

            logger.debug(f"API status: {api_response.status_code}")
            if api_response.status_code == 401:
                logger.error("API access denied (401). Check your headers and cookies if persistent.")
                return None
            api_response.raise_for_status() # Use api_response here
            if not api_response.content:    # Use api_response here
                logger.error("Empty API response received")
                return None
            data = api_response.json()      # Use api_response here
            logger.success("Fetched FII/DII data from API")
            logger.debug(f"API response snippet: {str(data)[:500]}...")
            return self._process_api_data(data)
        except Exception as e:
            logger.error(f"API scrape failed: {e}")
            return None

    def _process_api_data(self, data: Union[list, dict]) -> Optional[pl.DataFrame]:
        """Process NSE API response with flexible structure handling"""
        try:
            if isinstance(data, list):
                records = data
            elif isinstance(data, dict):
                if 'fiiDii' in data:
                    records = data['fiiDii']
                else:
                    records = data.get('data', [])
            else:
                logger.warning(f"Unexpected API response type: {type(data)}")
                return pl.DataFrame(schema={"date": pl.Date, "fii_buy_cr": pl.Float64, "fii_sell_cr": pl.Float64, "fii_net_cr": pl.Float64, "dii_buy_cr": pl.Float64, "dii_sell_cr": pl.Float64, "dii_net_cr": pl.Float64})

            if not records:
                logger.warning("No records in API response")
                return pl.DataFrame(schema={"date": pl.Date, "fii_buy_cr": pl.Float64, "fii_sell_cr": pl.Float64, "fii_net_cr": pl.Float64, "dii_buy_cr": pl.Float64, "dii_sell_cr": pl.Float64, "dii_net_cr": pl.Float64})

            df = pl.DataFrame(records)

            column_mapping = {
                "tradedDate": "date", "tradeDate": "date", "date": "date",
                "fiiBuy": "fii_buy_cr", "fiiPurchaseValue": "fii_buy_cr", "fii_buy": "fii_buy_cr",
                "fiiSell": "fii_sell_cr", "fiiSalesValue": "fii_sell_cr", "fii_sell": "fii_sell_cr",
                "fiiNet": "fii_net_cr", "fii_net": "fii_net_cr",
                "diiBuy": "dii_buy_cr", "diiPurchaseValue": "dii_buy_cr", "dii_buy": "dii_buy_cr",
                "diiSell": "dii_sell_cr", "diiSalesValue": "dii_sell_cr", "dii_sell": "dii_sell_cr",
                "diiNet": "dii_net_cr", "dii_net": "dii_net_cr"
            }

            # Rename columns based on mapping
            current_columns = df.columns
            df = df.rename({orig_col: column_mapping[orig_col]
                            for orig_col in current_columns if orig_col in column_mapping})

            # Parse date column
            if 'date' in df.columns:
                df = df.with_columns(
                    pl.col("date").str.strptime(pl.Date, "%d-%b-%Y", strict=False)
                )

            # Convert numeric columns - initial pass for all potential numeric columns
            potential_numeric_cols = [
                'fii_buy_cr', 'fii_sell_cr', 'fii_net_cr',
                'dii_buy_cr', 'dii_sell_cr', 'dii_net_cr'
            ]
            for col_name in potential_numeric_cols:
                if col_name in df.columns:  # If column exists from original data + renaming
                    df = df.with_columns(
                        pl.col(col_name)
                        .str.replace_all(r"[, ]", "")  # Remove commas and spaces
                        .replace("", None)  # Replace empty strings with null
                        .cast(pl.Float64, strict=False)  # Cast to float, invalid become null
                        .fill_null(0.0)  # Fill actual nulls (from cast error or original) with 0.0
                        .alias(col_name)
                    )

            # Ensure base 'buy' and 'sell' columns exist, defaulting to 0.0 if not present after initial processing
            base_buy_sell_cols = ['fii_buy_cr', 'fii_sell_cr', 'dii_buy_cr', 'dii_sell_cr']
            for col_name in base_buy_sell_cols:
                if col_name not in df.columns:
                    df = df.with_columns(pl.lit(0.0).cast(pl.Float64).alias(col_name))

            # Calculate net values. This will overwrite any 'fii_net_cr' or 'dii_net_cr' from source
            # if buy/sell columns are available. This is generally desired for consistency.
            df = df.with_columns(
                (pl.col("fii_buy_cr") - pl.col("fii_sell_cr")).alias("fii_net_cr")
            )
            df = df.with_columns(
                (pl.col("dii_buy_cr") - pl.col("dii_sell_cr")).alias("dii_net_cr")
            )

            # Ensure all expected final columns are present, adding them with 0.0 if missing
            # This is a final safety net, though above logic should create them.
            all_expected_numeric_cols = ['fii_buy_cr', 'fii_sell_cr', 'fii_net_cr', 'dii_buy_cr', 'dii_sell_cr', 'dii_net_cr']
            for col_name in all_expected_numeric_cols:
                if col_name not in df.columns: # Should ideally not happen if above logic is complete
                    df = df.with_columns(pl.lit(0.0).cast(pl.Float64).alias(col_name))

            # Ensure date column exists
            if 'date' not in df.columns:
                 df = df.with_columns(pl.lit(None).cast(pl.Date).alias("date"))

            # Define final column order and select
            final_cols = ['date', 'fii_buy_cr', 'fii_sell_cr', 'fii_net_cr', 'dii_buy_cr', 'dii_sell_cr', 'dii_net_cr']
            df = df.select(final_cols)

            logger.info(f"Processed {len(df)} records from API")
            return df
        except Exception as e:
            logger.exception(f"API data processing failed: {e}")
            return pl.DataFrame(schema={"date": pl.Date, "fii_buy_cr": pl.Float64, "fii_sell_cr": pl.Float64, "fii_net_cr": pl.Float64, "dii_buy_cr": pl.Float64, "dii_sell_cr": pl.Float64, "dii_net_cr": pl.Float64})

    async def scrape_with_playwright(self) -> Optional[pl.DataFrame]:
        """Scrape using Playwright with anti-automation flags and robust error handling"""
        retries = 0
        http_versions = ["2", "1.1"]
        while retries < self.max_retries:
            for http_version in http_versions:
                try:
                    user_agent = random.choice(self.user_agents)
                    logger.debug(f"Using Chromium with HTTP/{http_version}")
                    async with async_playwright() as p:
                        browser = await p.chromium.launch(
                            headless=True,
                            args=[
                                "--disable-blink-features=AutomationControlled",
                                "--disable-extensions",
                                "--disable-gpu",
                                "--no-sandbox",
                                f"--http-version={http_version}"
                            ]
                        )
                        context = await browser.new_context(
                            user_agent=user_agent,
                            viewport={'width': 1280, 'height': 800},
                            bypass_csp=True,
                            ignore_https_errors=True
                        )
                        page = await context.new_page()
                        page.set_default_timeout(30000)
                        await page.set_extra_http_headers({
                            "Accept-Language": "en-US,en;q=0.9",
                            "Cache-Control": "no-cache"
                        })
                        await page.goto(
                            self.html_url, # Use configured html_url
                            wait_until="domcontentloaded",
                            timeout=40000
                        )
                        try:
                            await page.wait_for_selector(self.HTML_TABLE_SELECTOR, timeout=15000) # Use configured selector
                        except:
                            logger.debug(f"{self.HTML_TABLE_SELECTOR} not found") # Adjusted log
                        content = await page.content()
                        await browser.close()
                        soup = BeautifulSoup(content, 'html.parser')
                        table = soup.find('table', class_='fii-dii-table')
                        if table:
                            return self._parse_html_table(table)
                        for selector in self.css_selectors:
                            table = soup.select_one(selector)
                            if table:
                                return self._parse_html_table(table)
                        logger.warning("No valid tables found in page")
                except (PlaywrightError, asyncio.TimeoutError) as e:
                    logger.warning(f"HTTP/{http_version} attempt failed: {str(e)[:100]}")
                except Exception as e:
                    logger.error(f"Unexpected error: {e}")
            retries += 1
            wait_time = self.retry_delay * (2 ** retries)
            logger.info(f"Retry {retries}/{self.max_retries} in {wait_time}s")
            await asyncio.sleep(wait_time)
        logger.error("Playwright scraping failed after all attempts")
        return None

    def _parse_html_table(self, table) -> Optional[pl.DataFrame]:
        """Parse HTML table into structured DataFrame"""
        try:
            headers = []
            header_row = table.find('tr')
            if header_row:
                headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
            header_count = {}
            unique_headers = []
            for header in headers:
                clean_header = re.sub(r'\s+', ' ', header).strip()
                if clean_header in header_count:
                    header_count[clean_header] += 1
                    unique_headers.append(f"{clean_header}_{header_count[clean_header]}")
                else:
                    header_count[clean_header] = 1
                    unique_headers.append(clean_header)
            rows = []
            for row in table.find_all('tr')[1:]:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                if row_data:
                    rows.append(row_data[:len(unique_headers)])
            if not rows:
                return None
            df = pl.DataFrame(rows, schema=unique_headers)

            column_mapping = {}
            for col in df.columns: # 'col' is the original header string from HTML
                col_text_lower = col.lower() # Use a new variable for the lowercased version for checks

                if 'date' in col_text_lower:
                    column_mapping[col] = 'date'
                # Check for NET first, as it's more specific and can contain buy/sell keywords
                elif 'fii' in col_text_lower and 'net' in col_text_lower:
                    column_mapping[col] = 'fii_net_cr'
                elif 'dii' in col_text_lower and 'net' in col_text_lower:
                    column_mapping[col] = 'dii_net_cr'
                # Then check for buy/sell
                elif 'fii' in col_text_lower and ('buy' in col_text_lower or 'purchase' in col_text_lower):
                    column_mapping[col] = 'fii_buy_cr'
                elif 'fii' in col_text_lower and ('sell' in col_text_lower or 'sales' in col_text_lower):
                    column_mapping[col] = 'fii_sell_cr'
                elif 'dii' in col_text_lower and ('buy' in col_text_lower or 'purchase' in col_text_lower):
                    column_mapping[col] = 'dii_buy_cr'
                elif 'dii' in col_text_lower and ('sell' in col_text_lower or 'sales' in col_text_lower):
                    column_mapping[col] = 'dii_sell_cr'

            # Apply renaming using the constructed mapping.
            # Handle potential duplicate target columns from mapping:
            # If multiple source columns map to the same target, polars rename will fail.
            # We need to ensure that column_mapping results in unique target names,
            # or handle this by selecting specific source columns if multiple map to one target.
            # For now, assume the first encountered mapping for a target name is preferred.
            final_renamed_cols = {}
            processed_source_cols = set()
            # Build a new mapping that ensures unique target names for rename
            temp_rename_mapping = {}
            # Prioritize specific mappings if there are known ambiguous headers
            # This is a simplified approach; a more robust one might involve scoring matches.
            for original_header, target_name in column_mapping.items():
                if original_header in df.columns and target_name not in temp_rename_mapping.values():
                    temp_rename_mapping[original_header] = target_name
                elif original_header in df.columns and target_name in temp_rename_mapping.values():
                    # A target name is already mapped. Log a warning or decide on priority.
                    logger.debug(f"HTML parsing: Target column '{target_name}' already mapped. Skipping duplicate mapping for '{original_header}'.")

            df = df.rename(temp_rename_mapping)

            # Ensure 'date' column exists before trying to parse it
            if 'date' not in df.columns and any('date' in c.lower() for c in unique_headers):
                # This part is less likely to be needed if temp_rename_mapping is robust
                pass

            if 'date' in df.columns:
                df = df.with_columns(
                    pl.coalesce([
                        pl.col("date").str.strptime(pl.Date, "%d %b %Y", strict=False),  # Try "26 Aug 2024"
                        pl.col("date").str.strptime(pl.Date, "%d-%b-%Y", strict=False) # Try "26-Aug-2024"
                    ]).alias("date")
                )

            # Process numeric columns that exist after renaming
            numeric_cols = [col for col in df.columns if col != 'date' and col in [
                'fii_buy_cr', 'fii_sell_cr', 'fii_net_cr',
                'dii_buy_cr', 'dii_sell_cr', 'dii_net_cr'
            ]] # Only process known numeric columns
            for col in numeric_cols:
                df = df.with_columns(
                    pl.col(col).str.replace_all(r'[^\d.]', '').cast(pl.Float64)
                )
            if 'fii_net_cr' not in df.columns and 'fii_buy_cr' in df.columns and 'fii_sell_cr' in df.columns:
                df = df.with_columns(
                    (pl.col("fii_buy_cr") - pl.col("fii_sell_cr")).alias("fii_net_cr")
                )
            if 'dii_net_cr' not in df.columns and 'dii_buy_cr' in df.columns and 'dii_sell_cr' in df.columns:
                df = df.with_columns(
                    (pl.col("dii_buy_cr") - pl.col("dii_sell_cr")).alias("dii_net_cr")
                )

            # At this point, df contains columns found and parsed from HTML.
            # Now ensure all required output columns exist, calculate net, and set order.

            target_columns_schema = {
                "date": pl.Date,
                "fii_buy_cr": pl.Float64, "fii_sell_cr": pl.Float64, "fii_net_cr": pl.Float64,
                "dii_buy_cr": pl.Float64, "dii_sell_cr": pl.Float64, "dii_net_cr": pl.Float64
            }
            final_ordered_cols = list(target_columns_schema.keys())

            # Ensure base buy/sell columns exist, defaulting to 0.0 if missing or unparseable
            # This loop also ensures they are Float64.
            for col_name in ["fii_buy_cr", "fii_sell_cr", "dii_buy_cr", "dii_sell_cr"]:
                if col_name in df.columns:
                    # Ensure it's float, fill nulls from failed cast with 0.0
                    df = df.with_columns(pl.col(col_name).cast(pl.Float64, strict=False).fill_null(0.0).alias(col_name))
                else:
                    df = df.with_columns(pl.lit(0.0).cast(pl.Float64).alias(col_name))

            # Calculate/overwrite net columns using the now guaranteed base columns
            df = df.with_columns((pl.col("fii_buy_cr") - pl.col("fii_sell_cr")).alias("fii_net_cr"))
            df = df.with_columns((pl.col("dii_buy_cr") - pl.col("dii_sell_cr")).alias("dii_net_cr"))

            # Ensure date column exists and is of correct type
            if "date" not in df.columns:
                df = df.with_columns(pl.lit(None).cast(pl.Date).alias("date"))
            else: # Ensure existing date column is of Date type
                 # If strptime failed resulting in all nulls, it might keep Utf8. Cast to ensure.
                if df["date"].dtype != pl.Date:
                    df = df.with_columns(pl.col("date").cast(pl.Date, strict=False)) # strict=False allows null propagation on error

            # Ensure any other missing final columns (like net_cr if somehow calculation was skipped) are added
            for col_name in final_ordered_cols:
                if col_name not in df.columns:
                    # This case should ideally be covered by above logic for specific columns
                    # For safety, add date as null if missing, others as 0.0
                    if col_name == "date":
                        df = df.with_columns(pl.lit(None).cast(pl.Date).alias(col_name))
                    else:
                        df = df.with_columns(pl.lit(0.0).cast(pl.Float64).alias(col_name))

            df = df.select(final_ordered_cols) # Select in fixed order
            return df
        except Exception as e:
            logger.exception(f"HTML table parsing failed: {e}") # Use logger.exception for stack trace
            target_columns_schema = { # Define schema for empty DataFrame on error
                "date": pl.Date, "fii_buy_cr": pl.Float64, "fii_sell_cr": pl.Float64,
                "fii_net_cr": pl.Float64, "dii_buy_cr": pl.Float64, "dii_sell_cr": pl.Float64,
                "dii_net_cr": pl.Float64
            }
            return pl.DataFrame(schema=target_columns_schema)

    def _generate_mock_data(self) -> pl.DataFrame:
        """Generate mock FII/DII data for fallback"""
        try:
            mock_data = []
            for i in range(5):
                trade_date = date.today() - timedelta(days=i)
                if trade_date.weekday() >= 5:
                    continue
                fii_buy = round(random.uniform(2000, 8000), 2)
                fii_sell = round(random.uniform(1800, 7500), 2)
                dii_buy = round(random.uniform(1500, 4000), 2)
                dii_sell = round(random.uniform(1400, 3800), 2)
                mock_data.append({
                    'date': trade_date,
                    'fii_buy_cr': fii_buy,
                    'fii_sell_cr': fii_sell,
                    'fii_net_cr': fii_buy - fii_sell,
                    'dii_buy_cr': dii_buy,
                    'dii_sell_cr': dii_sell,
                    'dii_net_cr': dii_buy - dii_sell
                })
            df = pl.DataFrame(mock_data)
            logger.warning(f"Generated {len(mock_data)} days of mock data")
            logger.warning("âš ï¸ WARNING: Using mock data as fallback. Check your scraping setup.")
            return df
        except Exception as e:
            logger.error(f"Mock data generation failed: {e}")
            return None

    async def scrape(self) -> Optional[pl.DataFrame]:
        """Main scraping method with fallback strategy"""
        logger.info("Trying official API...")
        data = await self.scrape_with_api()
        if data is not None and not data.is_empty():
            return data
        logger.info("API failed, trying Playwright...")
        data = await self.scrape_with_playwright()
        if data is not None and not data.is_empty():
            return data
        logger.error("All scraping methods failed, using mock data")
        return self._generate_mock_data()

    def update_database(self, df: pl.DataFrame) -> bool:
        """UPSERT data into DuckDB"""
        if df is None or df.is_empty():
            logger.warning("No data to update")
            return False
        try:
            # Explicitly set read_only=False to match test assertion expectations
            with duckdb.connect(self.db_path, read_only=False) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS institutional_flows (
                        date DATE PRIMARY KEY,
                        fii_buy_cr DOUBLE,
                        fii_sell_cr DOUBLE,
                        fii_net_cr DOUBLE,
                        dii_buy_cr DOUBLE,
                        dii_sell_cr DOUBLE,
                        dii_net_cr DOUBLE
                    )
                """)
                conn.register("temp_fii_dii", df)
                conn.execute("""
                    INSERT INTO institutional_flows BY NAME
                    SELECT * FROM temp_fii_dii
                    ON CONFLICT (date) DO UPDATE SET
                        fii_buy_cr = EXCLUDED.fii_buy_cr,
                        fii_sell_cr = EXCLUDED.fii_sell_cr,
                        fii_net_cr = EXCLUDED.fii_net_cr,
                        dii_buy_cr = EXCLUDED.dii_buy_cr,
                        dii_sell_cr = EXCLUDED.dii_sell_cr,
                        dii_net_cr = EXCLUDED.dii_net_cr
                """)
                conn.execute("""
                    CREATE OR REPLACE VIEW v_institutional_trends AS
                    SELECT
                        date,
                        fii_net_cr,
                        dii_net_cr,
                        SUM(fii_net_cr) OVER (
                            ORDER BY date
                            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
                        ) AS fii_30d_roll,
                        SUM(dii_net_cr) OVER (
                            ORDER BY date
                            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
                        ) AS dii_30d_roll
                    FROM institutional_flows
                    ORDER BY date DESC
                """)
                logger.success(f"Updated {len(df)} records")
                return True
        except Exception as e:
            logger.error(f"Database update failed: {e}")
            return False

async def main():
    """Main async workflow"""
    logger.info("ðŸš€ Starting FII/DII tracker")
    start_time = time.time()
    scraper = NSE_FII_DII_Scraper()
    data = await scraper.scrape()
    if data is not None:
        scraper.update_database(data)
    else:
        logger.error("Scraping failed completely")
    duration = time.time() - start_time
    logger.info(f"âœ… Tracker completed in {duration:.2f} seconds")

    if data is not None: # This 'data' is the result of scraper.scrape()
        logger.info(f"Data sample:\n{data.head(2)}") # Log sample of data that was processed
        try:
            with duckdb.connect(scraper.db_path) as conn:
                latest_df = conn.execute("""
                    SELECT date, fii_net_cr, dii_net_cr
                    FROM institutional_flows
                    ORDER BY date DESC
                    LIMIT 1
                """).fetchdf() # fetchdf() returns a pandas DataFrame

                # Check for pandas DataFrame and if it's not empty
                if latest_df is not None and not latest_df.empty:
                    # Convert pandas DataFrame to dictionary for logging
                    # latest_df.to_dict(orient='records') returns a list of dicts
                    logger.success(f"Latest record in DB: {latest_df.to_dict(orient='records')[0]}")
                else:
                    logger.warning("No latest record found in institutional_flows table or table is empty.")
        except Exception as e:
            logger.error(f"Failed to fetch or log latest record from DB: {e}")

if __name__ == "__main__":
    asyncio.run(main())
